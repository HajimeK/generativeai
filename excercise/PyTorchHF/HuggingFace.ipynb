{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive (88.68%)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load a pre-trained sentiment analysis model\n",
    "model_name = \"textattack/bert-base-uncased-imdb\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Tokenize the input sequence\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer(\"I love Generative AI\", return_tensors=\"pt\")\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs).logits\n",
    "    probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities)\n",
    "\n",
    "# Display sentiment result\n",
    "if predicted_class == 1:\n",
    "    print(f\"Sentiment: Positive ({probabilities[0][1] * 100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"Sentiment: Negative ({probabilities[0][0] * 100:.2f}%)\")\n",
    "# Sentiment: Positive (88.68%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movie Centiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 7.81k/7.81k [00:00<?, ?B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/plain_text to file://C:/Users/smeck/.cache/huggingface/datasets/parquet/plain_text-745310791ff4d097/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 21.0M/21.0M [00:02<00:00, 9.21MB/s]\n",
      "Downloading data: 100%|██████████| 20.5M/20.5M [00:02<00:00, 8.83MB/s]\n",
      "Downloading data files: 100%|██████████| 2/2 [00:06<00:00,  3.22s/it]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 89.17it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "ename": "ExpectedMoreSplits",
     "evalue": "{'unsupervised'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mExpectedMoreSplits\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m HTML, display\n\u001b[0;32m      4\u001b[0m \u001b[39m# Load the IMDB dataset, which contains movie reviews\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# and sentiment labels (positive or negative)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mimdb\u001b[39;49m\u001b[39m\"\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m, download_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mforce_redownload\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      8\u001b[0m \u001b[39m# Fetch a revie from the training set\u001b[39;00m\n\u001b[0;32m      9\u001b[0m review_number \u001b[39m=\u001b[39m \u001b[39m42\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\smeck\\.conda\\envs\\cudaenv\\lib\\site-packages\\datasets\\load.py:1797\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1794\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[0;32m   1796\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> 1797\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[0;32m   1798\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[0;32m   1799\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[0;32m   1800\u001b[0m     verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[0;32m   1801\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[0;32m   1802\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[0;32m   1803\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   1804\u001b[0m )\n\u001b[0;32m   1806\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   1807\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[0;32m   1808\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[0;32m   1809\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\smeck\\.conda\\envs\\cudaenv\\lib\\site-packages\\datasets\\builder.py:890\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    888\u001b[0m     \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    889\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n\u001b[1;32m--> 890\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_and_prepare(\n\u001b[0;32m    891\u001b[0m         dl_manager\u001b[39m=\u001b[39mdl_manager,\n\u001b[0;32m    892\u001b[0m         verification_mode\u001b[39m=\u001b[39mverification_mode,\n\u001b[0;32m    893\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_split_kwargs,\n\u001b[0;32m    894\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[0;32m    895\u001b[0m     )\n\u001b[0;32m    896\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[0;32m    897\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[1;32mc:\\Users\\smeck\\.conda\\envs\\cudaenv\\lib\\site-packages\\datasets\\builder.py:1003\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m   1000\u001b[0m     dl_manager\u001b[39m.\u001b[39mmanage_extracted_files()\n\u001b[0;32m   1002\u001b[0m \u001b[39mif\u001b[39;00m verification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS \u001b[39mor\u001b[39;00m verification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS:\n\u001b[1;32m-> 1003\u001b[0m     verify_splits(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfo\u001b[39m.\u001b[39;49msplits, split_dict)\n\u001b[0;32m   1005\u001b[0m \u001b[39m# Update the info object with the splits.\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits \u001b[39m=\u001b[39m split_dict\n",
      "File \u001b[1;32mc:\\Users\\smeck\\.conda\\envs\\cudaenv\\lib\\site-packages\\datasets\\utils\\info_utils.py:91\u001b[0m, in \u001b[0;36mverify_splits\u001b[1;34m(expected_splits, recorded_splits)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(expected_splits) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(recorded_splits)) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 91\u001b[0m     \u001b[39mraise\u001b[39;00m ExpectedMoreSplits(\u001b[39mstr\u001b[39m(\u001b[39mset\u001b[39m(expected_splits) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(recorded_splits)))\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(recorded_splits) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(expected_splits)) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m UnexpectedSplits(\u001b[39mstr\u001b[39m(\u001b[39mset\u001b[39m(recorded_splits) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(expected_splits)))\n",
      "\u001b[1;31mExpectedMoreSplits\u001b[0m: {'unsupervised'}"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Load the IMDB dataset, which contains movie reviews\n",
    "# and sentiment labels (positive or negative)\n",
    "dataset = load_dataset(\"imdb\", split=\"train\", download_mode=\"force_redownload\")\n",
    "\n",
    "# Fetch a revie from the training set\n",
    "review_number = 42\n",
    "sample_review = dataset[\"train\"][review_number]\n",
    "\n",
    "display(HTML(sample_review[\"text\"][:450] + \"...\"))\n",
    "# WARNING: This review contains SPOILERS. Do not read if you don't want some points revealed to you before you watch the\n",
    "# film.\n",
    "#\n",
    "# With a cast like this, you wonder whether or not the actors and actresses knew exactly what they were getting into. Did they\n",
    "# see the script and say, `Hey, Close Encounters of the Third Kind was such a hit that this one can't fail.' Unfortunately, it does.\n",
    "# Did they even think to check on the director's credentials...\n",
    "\n",
    "if sample_review[\"label\"] == 1:\n",
    "    print(\"Sentiment: Positive\")\n",
    "else:\n",
    "    print(\"Sentiment: Negative\")\n",
    "# Sentiment: Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|██████████| 483/483 [00:00<00:00, 56.7kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 268M/268M [00:32<00:00, 8.35MB/s] \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 2.30MB/s]\n",
      "Downloading tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 3.01kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/plain_text to file://C:/Users/smeck/.cache/huggingface/datasets/parquet/plain_text-745310791ff4d097/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 942.86it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 236.36it/s]\n",
      "                                                                                        \r"
     ]
    },
    {
     "ename": "ExpectedMoreSplits",
     "evalue": "{'unsupervised'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mExpectedMoreSplits\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_function\u001b[39m(examples):\n\u001b[0;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer(examples[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m], padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 17\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mimdb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     18\u001b[0m tokenized_datasets \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mmap(tokenize_function, batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m     21\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m,\n\u001b[0;32m     22\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./results\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     23\u001b[0m     learning_rate\u001b[39m=\u001b[39m\u001b[39m2e-5\u001b[39m,\n\u001b[0;32m     24\u001b[0m     num_train_epochs\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[0;32m     25\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\smeck\\.conda\\envs\\cudaenv\\lib\\site-packages\\datasets\\load.py:1797\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1794\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[0;32m   1796\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> 1797\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[0;32m   1798\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[0;32m   1799\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[0;32m   1800\u001b[0m     verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[0;32m   1801\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[0;32m   1802\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[0;32m   1803\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   1804\u001b[0m )\n\u001b[0;32m   1806\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   1807\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[0;32m   1808\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[0;32m   1809\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\smeck\\.conda\\envs\\cudaenv\\lib\\site-packages\\datasets\\builder.py:890\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    888\u001b[0m     \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    889\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n\u001b[1;32m--> 890\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_and_prepare(\n\u001b[0;32m    891\u001b[0m         dl_manager\u001b[39m=\u001b[39mdl_manager,\n\u001b[0;32m    892\u001b[0m         verification_mode\u001b[39m=\u001b[39mverification_mode,\n\u001b[0;32m    893\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_split_kwargs,\n\u001b[0;32m    894\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[0;32m    895\u001b[0m     )\n\u001b[0;32m    896\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[0;32m    897\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[1;32mc:\\Users\\smeck\\.conda\\envs\\cudaenv\\lib\\site-packages\\datasets\\builder.py:1003\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m   1000\u001b[0m     dl_manager\u001b[39m.\u001b[39mmanage_extracted_files()\n\u001b[0;32m   1002\u001b[0m \u001b[39mif\u001b[39;00m verification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS \u001b[39mor\u001b[39;00m verification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS:\n\u001b[1;32m-> 1003\u001b[0m     verify_splits(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfo\u001b[39m.\u001b[39;49msplits, split_dict)\n\u001b[0;32m   1005\u001b[0m \u001b[39m# Update the info object with the splits.\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits \u001b[39m=\u001b[39m split_dict\n",
      "File \u001b[1;32mc:\\Users\\smeck\\.conda\\envs\\cudaenv\\lib\\site-packages\\datasets\\utils\\info_utils.py:91\u001b[0m, in \u001b[0;36mverify_splits\u001b[1;34m(expected_splits, recorded_splits)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(expected_splits) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(recorded_splits)) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 91\u001b[0m     \u001b[39mraise\u001b[39;00m ExpectedMoreSplits(\u001b[39mstr\u001b[39m(\u001b[39mset\u001b[39m(expected_splits) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(recorded_splits)))\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(recorded_splits) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(expected_splits)) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m UnexpectedSplits(\u001b[39mstr\u001b[39m(\u001b[39mset\u001b[39m(recorded_splits) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(expected_splits)))\n",
      "\u001b[1;31mExpectedMoreSplits\u001b[0m: {'unsupervised'}"
     ]
    }
   ],
   "source": [
    "from transformers import (DistilBertForSequenceClassification,\n",
    "    DistilBertTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=64,\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
