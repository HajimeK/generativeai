{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvBHYcpd4v_1"
   },
   "source": [
    "# 環境セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8pzPvghP5TBs"
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOGETHER_API_KEY =  open(\"../../api_keys/together.key\", \"rt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT = 'https://api.together.xyz/inference'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パラメータの設定\n",
    "\n",
    "LLMの出力を安定的なものとするか、ランダム性の高いものとするかはパラメータ値を調整することで可能です。\n",
    "\n",
    "以下のパラメータを変化させ、LLMモデルの出力の変化を見ます。\n",
    "\n",
    "| パラメータ         | 説明 |\n",
    "|-------------------|------|\n",
    "| Model | GPT3とかdavinciなどのモデルを指定 |\n",
    "| TEMPERATURE       | サンプリング温度と呼ばれるLLMが推論するトークンの出現確率分布を決めるパラメータで、0から1の間を設定する。<p>値が1に近いほど、分布の形はフラットとなる。つまり、TEMPERATUREの値が高いほど出力はランダムになり、値が低ければ出力は決定的なものになる。<p>0に設定すると、モデルは対数確率を使用して、特定のしきい値に達するまで温度を自動的に上昇させます。 |\n",
    "| MAX_TOKENS        | 生成されるトークンの最大長。 |\n",
    "| TOP_P             | トークン出現確率分布のどこまでをトークン採用範囲とするかコントロールするためのパラメータ |\n",
    "| REPITIION_PENALTY | 繰り返しに対するペナルティ  |\n",
    "\n",
    "ほかにもさまざまなパラメータがあります。https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfigを参照にしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8IF2Fa6tg_w1"
   },
   "outputs": [],
   "source": [
    "# Decoding parameters\n",
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS = 512\n",
    "TOP_P = 1.0\n",
    "REPITIION_PENALTY = 1.0\n",
    "\n",
    "# https://huggingface.co/meta-llama/Llama-2-7b-hf\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together.xyzに対しクエリを投げる関数です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gt80J3Zy5rM4"
   },
   "outputs": [],
   "source": [
    "def query_together_endpoint(prompt):\n",
    "    return requests.post(ENDPOINT, json={\n",
    "        \"model\": \"togethercomputer/llama-2-7b-chat\",\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"prompt\": prompt,\n",
    "        \"request_type\": \"language-model-inference\",\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"top_p\": TOP_P,\n",
    "        \"repetition_penalty\": REPITIION_PENALTY,\n",
    "        \"stop\": [\n",
    "            E_INST,\n",
    "            E_SYS\n",
    "        ],\n",
    "        \"negative_prompt\": \"\",\n",
    "    }, headers={\n",
    "        \"Authorization\": f\"Bearer {TOGETHER_API_KEY}\",\n",
    "    }).json()['output']['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXJZqpiV1_Pp"
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "p-piCAwug9fN"
   },
   "outputs": [],
   "source": [
    "def query_model(prompt,  trigger = None, verbose=True, **kwargs):\n",
    "    inst_prompt = f\"{B_INST} {prompt} {E_INST}\"\n",
    "    if trigger:\n",
    "        inst_prompt = inst_prompt + trigger\n",
    "    generation = query_together_endpoint(inst_prompt)\n",
    "    if verbose:\n",
    "        print(f\"*** Prompt ***\\n{inst_prompt}\")\n",
    "        print(f\"*** Generation ***\\n{generation}\")\n",
    "    return generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZhFzjfQ2CAg"
   },
   "source": [
    "## System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "h8w88wHjt5X2"
   },
   "outputs": [],
   "source": [
    "ANSWER_STAGE = \"ユーザーの質問に直接答えます。\"\n",
    "REASONING_STAGE = \"答えを見つけるための推論を段階的に説明します。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "e2Oxy5RTs20Z"
   },
   "outputs": [],
   "source": [
    "# System prompt can be constructed in two ways:\n",
    "# 1) Answering the question first or\n",
    "# 2) Providing the reasoning first\n",
    "\n",
    "# Similar ablation performed in \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\"\n",
    "# https://arxiv.org/pdf/2201.11903.pdf\n",
    "SYSTEM_PROMPT_TEMPLATE = \"\"\"{b_sys}次の形式を使用してユーザーの質問に回答します:\n",
    "1) {stage_1}\n",
    "2) {stage_2}{e_sys}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain of thought trigger from \"Large Language Models are Zero-Shot Reasoners\"\n",
    "# https://arxiv.org/abs/2205.11916\n",
    "COT_TRIGGER = \"\\n\\nA: Lets think step by step:\"\n",
    "A_TRIGGER = \"\\n\\nA:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KT7pJzdi2M-8"
   },
   "source": [
    "## User prompt for our task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "iEUcXYNckT6d"
   },
   "outputs": [],
   "source": [
    "user_prompt_template = \"Q: Llama 2 には、{atten_window} トークンのコンテキスト ウィンドウがあります。 \\\n",
    "LLM 応答用にそのうちの {max_token} を予約している場合、\\\n",
    "システム プロンプトは {sys_prompt_len} を使用します。\\\n",
    "思考連鎖トリガーは {trigger_len} のみを使用します, \\\n",
    "最後に、会話履歴は {convo_history_len} を使用します。 \\\n",
    "ユーザープロンプトにはいくつ使用できますか?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "atten_window = 4096\n",
    "max_token = 512\n",
    "sys_prompt_len = 124\n",
    "trigger_len = 11\n",
    "convo_history_len = 390\n",
    "\n",
    "user_prompt = user_prompt_template.format(\n",
    "    atten_window=atten_window,\n",
    "    max_token=max_token,\n",
    "    sys_prompt_len=sys_prompt_len,\n",
    "    trigger_len=trigger_len,\n",
    "    convo_history_len=convo_history_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYozeQNor7fd",
    "outputId": "240f1fc1-fb29-4ec8-abd5-1d233845746d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3059"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desired_numeric_answer = atten_window - max_token - sys_prompt_len - trigger_len - convo_history_len\n",
    "desired_numeric_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7rs_lWP2VWF"
   },
   "source": [
    "## Testing the prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User prompt only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTOKsW82IIxP",
    "outputId": "2e918314-58d9-40b4-f5f0-f02fe9e00817"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Prompt ***\n",
      "[INST] Q: Llama 2 には、4096 トークンのコンテキスト ウィンドウがあります。 LLM 応答用にそのうちの 512 を予約している場合、システム プロンプトは 124 を使用します。思考連鎖トリガーは 11 のみを使用します, 最後に、会話履歴は 390 を使用します。 ユーザープロンプトにはいくつ使用できますか? [/INST]\n",
      "*** Generation ***\n",
      " If Llama 2 has a context window of 4096 tokens and reserves 512 tokens for LLM responses, the system prompts will use 124 tokens, the thinking chain triggers will use 11 triggers, and the conversation history will use 390 tokens.\n",
      "\n",
      "As for the user prompts, you can use up to 4096 - 512 = 3584 tokens.\n"
     ]
    }
   ],
   "source": [
    "r = query_model(user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User prompt + system prompt v1: answering first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmkqUpP7J5Zw",
    "outputId": "681caacf-c691-4764-f7b0-d27e765ab72c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Prompt ***\n",
      "[INST] <<SYS>>\n",
      "次の形式を使用してユーザーの質問に回答します:\n",
      "1) ユーザーの質問に直接答えます。\n",
      "2) 答えを見つけるための推論を段階的に説明します。\n",
      "<</SYS>>\n",
      "\n",
      "Q: Llama 2 には、4096 トークンのコンテキスト ウィンドウがあります。 LLM 応答用にそのうちの 512 を予約している場合、システム プロンプトは 124 を使用します。思考連鎖トリガーは 11 のみを使用します, 最後に、会話履歴は 390 を使用します。 ユーザープロンプトにはいくつ使用できますか? [/INST]\n",
      "*** Generation ***\n",
      " Sure, I'd be happy to help you with your question!\n",
      "\n",
      "Based on the information provided, it seems that the system is using 124 as the prompt for the LLM response. The user prompt is limited to 390 characters, and the system is using only 11 thoughts for the chain trigger.\n",
      "\n",
      "Therefore, the user can use a maximum of 390 - 11 = 379 characters in their prompt for the LLM response.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = SYSTEM_PROMPT_TEMPLATE.format(\n",
    "    b_sys = B_SYS,\n",
    "    stage_1=ANSWER_STAGE,\n",
    "    stage_2=REASONING_STAGE,\n",
    "    e_sys=E_SYS\n",
    ")\n",
    "prompt = \"\".join([system_prompt, user_prompt])\n",
    "\n",
    "r2 = query_model(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User prompt + system prompt v2: reasoning first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfPHZ9v-tnPn",
    "outputId": "bfeac801-a82b-430f-a700-accd443ca775"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Prompt ***\n",
      "[INST] <<SYS>>\n",
      "次の形式を使用してユーザーの質問に回答します:\n",
      "1) 答えを見つけるための推論を段階的に説明します。\n",
      "2) ユーザーの質問に直接答えます。\n",
      "<</SYS>>\n",
      "\n",
      "Q: Llama 2 には、4096 トークンのコンテキスト ウィンドウがあります。 LLM 応答用にそのうちの 512 を予約している場合、システム プロンプトは 124 を使用します。思考連鎖トリガーは 11 のみを使用します, 最後に、会話履歴は 390 を使用します。 ユーザープロンプトにはいくつ使用できますか? [/INST]\n",
      "*** Generation ***\n",
      " Sure, I'd be happy to help you with your question!\n",
      "\n",
      "1. To explain the reasoning behind the system prompts:\n",
      "\n",
      "The system prompts in this scenario are designed to help the LLM generate a response to the user's question. The prompts are based on the context of the conversation and the user's previous input.\n",
      "\n",
      "The first prompt, \"124\", is used to reserve 512 tokens for the LLM's response. This is because the LLM needs a certain amount of tokens to generate a response that is contextually relevant and coherent. By reserving 512 tokens, the system ensures that the LLM has enough resources to generate a high-quality response.\n",
      "\n",
      "The second prompt, \"11\", is used to trigger the LLM's thinking process. This prompt is based on the user's previous input, which suggests that they are interested in learning more about the topic. By using the \"11\" prompt, the system can encourage the LLM to generate a response that is relevant to the user's interests.\n",
      "\n",
      "The third prompt, \"390\", is used to store the conversation history. This is important because the LLM needs to be able to access the previous conversation to generate a response that is contextually relevant. By storing the conversation history, the system can ensure that the LLM has access to the necessary information to generate an informed response.\n",
      "\n",
      "2. To answer the user's question directly:\n",
      "\n",
      "Based on the system prompts provided, the user can use up to 11 prompts in their conversation with the LLM. These prompts are designed to help the LLM generate a response that is contextually relevant and coherent, and they are based on the user's previous input and interests. By using these prompts, the user can encourage the LLM to generate a response that is tailored to their needs and preferences.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = SYSTEM_PROMPT_TEMPLATE.format(b_sys = B_SYS, stage_1=REASONING_STAGE, stage_2=ANSWER_STAGE, e_sys=E_SYS)\n",
    "prompt = \"\".join([system_prompt, user_prompt])\n",
    "\n",
    "r3 = query_model(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3059"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3584 - (124 + 11 + 390)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User prompt + cot trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Prompt ***\n",
      "[INST] Q: Llama 2 には、4096 トークンのコンテキスト ウィンドウがあります。 LLM 応答用にそのうちの 512 を予約している場合、システム プロンプトは 124 を使用します。思考連鎖トリガーは 11 のみを使用します, 最後に、会話履歴は 390 を使用します。 ユーザープロンプトにはいくつ使用できますか? [/INST]\n",
      "\n",
      "A: Lets think step by step:\n",
      "*** Generation ***\n",
      "\n",
      "\n",
      "1. Llama 2 has a context window of 4096 tokens.\n",
      "2. The system reserves 512 tokens for LLM responses.\n",
      "3. The system uses prompt 124 for LLM responses.\n",
      "4. The system uses trigger 11 for LLM responses.\n",
      "5. The system uses chat history 390 for LLM responses.\n",
      "\n",
      "Now, let's calculate the number of user prompts that can be used:\n",
      "\n",
      "4096 - 512 = 3584 tokens available for user prompts\n",
      "\n",
      "So, the user can use up to 3584 tokens for their prompts.\n"
     ]
    }
   ],
   "source": [
    "r4 = query_model(user_prompt, trigger=COT_TRIGGER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User prompt + \"A:\" trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Prompt ***\n",
      "[INST] Q: Llama 2 には、4096 トークンのコンテキスト ウィンドウがあります。 LLM 応答用にそのうちの 512 を予約している場合、システム プロンプトは 124 を使用します。思考連鎖トリガーは 11 のみを使用します, 最後に、会話履歴は 390 を使用します。 ユーザープロンプトにはいくつ使用できますか? [/INST]\n",
      "\n",
      "A:\n",
      "*** Generation ***\n",
      "If Llama 2 has a context window of 4096 tokens and reserves 512 tokens for LLM responses, the system prompt will use 124 tokens, the thinking chain trigger will use 11 tokens, and the conversation history will use 390 tokens.\n",
      "\n",
      "As for the user prompt, you can use up to 4096 - 512 = 3584 tokens.\n"
     ]
    }
   ],
   "source": [
    "r5 = query_model(user_prompt, trigger=A_TRIGGER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOiW36Ll4W/LJq40/BjGEnk",
   "include_colab_link": true,
   "mount_file_id": "1SkBFwV9AhTt8ymXpNk2b-7ehiq-TxEb4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
